{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import grid2op\n",
    "from grid2op.Agent import DoNothingAgent, BaseAgent\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "max_iter = 100  # to make computation much faster we will only consider 100 time steps\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Is this environment suitable for redispatching: True\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "env = grid2op.make(\"rte_case14_redisp\", test=False)\n",
    "print(\"Is this environment suitable for redispatching: {}\".format(env.redispatching_unit_commitment_availble))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True,  True, False, False,  True])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 17
    }
   ],
   "source": [
    "env.gen_redispatchable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='step'), FloatProgress(value=0.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd8d57a426c64d26b95084c061eaeef2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "\nThe cumulative reward with this agent is 121566\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "agent = DoNothingAgent(env.action_space)\n",
    "done = False\n",
    "reward = env.reward_range[0]\n",
    "\n",
    "env.set_id(0)  # make sure to evaluate the models on the same experiments\n",
    "obs = env.reset()\n",
    "cum_reward = 0\n",
    "nrow = env.chronics_handler.max_timestep() if max_iter <= 0 else max_iter\n",
    "gen_p = np.zeros((nrow, env.n_gen))\n",
    "gen_p_setpoint = np.zeros((nrow, env.n_gen))\n",
    "load_p = np.zeros((nrow, env.n_load))\n",
    "rho = np.zeros((nrow, env.n_line))\n",
    "i = 0\n",
    "with tqdm(total=max_iter, desc=\"step\") as pbar:\n",
    "    while not done:\n",
    "        act = agent.act(obs, reward, done)\n",
    "        obs, reward, done, info = env.step(act)\n",
    "        data_generator = env.chronics_handler.real_data.data\n",
    "        gen_p_setpoint[i,:] = data_generator.prod_p[data_generator.current_index, :]\n",
    "        gen_p[i,:] = obs.prod_p\n",
    "        load_p[i,:] = obs.load_p\n",
    "        rho[i,:] = obs.rho\n",
    "        cum_reward += reward\n",
    "        i += 1\n",
    "        pbar.update(1)\n",
    "        if i >= max_iter:\n",
    "            break\n",
    "print(\"The cumulative reward with this agent is {:.0f}\".format(cum_reward))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='step'), FloatProgress(value=0.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70cedc78eefb4d798ac7c1b0d179ff34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "\nThe cumulative reward with this agent is 97435\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class GreedyEconomic(BaseAgent):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__(action_space)\n",
    "        self.do_nothing = action_space()\n",
    "        \n",
    "    def act(self, obs, reward, done):\n",
    "        act = self.do_nothing\n",
    "        if obs.prod_p[0] < obs.gen_pmax[0] - 1 and \\\n",
    "        obs.target_dispatch[0] < (obs.gen_pmax[0] - obs.gen_max_ramp_up[0]) - 1 and\\\n",
    "        obs.prod_p[0] > 0.:\n",
    "            # if the cheapest generator is significantly bellow its maximum cost\n",
    "            if obs.target_dispatch[0] < obs.gen_pmax[0]:\n",
    "                #in theory i can still ask for more\n",
    "                act = env.action_space({\"redispatch\": [(0, obs.gen_max_ramp_up[0])]})\n",
    "        return act\n",
    "    \n",
    "agent = GreedyEconomic(env.action_space)\n",
    "done = False\n",
    "reward = env.reward_range[0]\n",
    "\n",
    "env.set_id(0) # reset the env to the same id\n",
    "obs = env.reset()\n",
    "cum_reward = 0\n",
    "nrow = env.chronics_handler.max_timestep() if max_iter <= 0 else max_iter\n",
    "gen_p = np.zeros((nrow, env.n_gen))\n",
    "gen_p_setpoint = np.zeros((nrow, env.n_gen))\n",
    "load_p = np.zeros((nrow, env.n_load))\n",
    "rho = np.zeros((nrow, env.n_line))\n",
    "i = 0\n",
    "with tqdm(total=max_iter, desc=\"step\") as pbar:\n",
    "    while not done:\n",
    "        act = agent.act(obs, reward, done)\n",
    "        obs, reward, done, info = env.step(act)\n",
    "#         print(\"act: {}\".format(act))\n",
    "#         print(\"info: {}\".format(info['exception']))\n",
    "#         if info['exception'] is not None:\n",
    "        if np.abs(np.sum(obs.actual_dispatch)) > 1e-2:\n",
    "            pdb.set_trace()\n",
    "        data_generator = env.chronics_handler.real_data.data\n",
    "        gen_p_setpoint[i,:] = data_generator.prod_p[data_generator.current_index, :]\n",
    "        gen_p[i,:] = obs.prod_p\n",
    "        load_p[i,:] = obs.load_p\n",
    "        rho[i,:] = obs.rho\n",
    "        cum_reward += reward\n",
    "        i += 1\n",
    "        pbar.update(1)\n",
    "        if i >= max_iter:\n",
    "            break\n",
    "print(\"The cumulative reward with this agent is {:.0f}\".format(cum_reward))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(-10.0, 706.4)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(env.reward_range)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}