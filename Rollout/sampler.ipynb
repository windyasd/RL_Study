{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import deque, OrderedDict\n",
    "from itertools import islice\n",
    "class BaseSampler(object):\n",
    "    def __init__(self,\n",
    "                 max_path_length,\n",
    "                 environment=None,\n",
    "                 policy=None,\n",
    "                 pool=None,\n",
    "                 store_last_n_paths=10):\n",
    "        self._max_path_length = max_path_length\n",
    "        self._store_last_n_paths = store_last_n_paths\n",
    "        self._last_n_paths = deque(maxlen=store_last_n_paths)\n",
    "\n",
    "        self.environment = environment\n",
    "        self.policy = policy\n",
    "        self.pool = pool\n",
    "\n",
    "    def initialize(self, environment, policy, pool):\n",
    "        self.environment = environment\n",
    "        self.policy = policy\n",
    "        self.pool = pool\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    def clear_last_n_paths(self):\n",
    "        self._last_n_paths.clear()\n",
    "\n",
    "    def get_last_n_paths(self, n=None):\n",
    "        if n is None:\n",
    "            n = self._store_last_n_paths\n",
    "\n",
    "        last_n_paths = tuple(islice(self._last_n_paths, None, n))\n",
    "\n",
    "        return last_n_paths\n",
    "\n",
    "    def sample(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def terminate(self):\n",
    "        self.environment.close()\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        diagnostics = OrderedDict({'pool-size': self.pool.size})\n",
    "        return diagnostics\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = {\n",
    "            key: value for key, value in self.__dict__.items()\n",
    "            if key not in (\n",
    "                'environment',\n",
    "                'policy',\n",
    "                'pool',\n",
    "                '_last_n_paths',\n",
    "                '_current_observation',\n",
    "                '_current_path',\n",
    "                '_is_first_step',\n",
    "            )\n",
    "        }\n",
    "\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "        self.environment = None\n",
    "        self.policy = None\n",
    "        self.pool = None\n",
    "        # TODO(hartikainen): Maybe try restoring these from the pool?\n",
    "        self._last_n_paths = deque(maxlen=self._store_last_n_paths)\n",
    "import numpy as np\n",
    "import tree\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, Callable\n",
    "from numbers import Number\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tree\n",
    "import abc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# replay buffer设定\n",
    "\n",
    "@dataclass\n",
    "class Field:\n",
    "    name: str\n",
    "    dtype: Union[str, np.dtype, tf.DType]\n",
    "    shape: Union[tuple, tf.TensorShape]\n",
    "    initializer: Callable = np.zeros\n",
    "    default_value: Number = 0.0\n",
    "\n",
    "INDEX_FIELDS = {\n",
    "    'episode_index_forwards': Field(\n",
    "        name='episode_index_forwards',\n",
    "        dtype='uint64',\n",
    "        shape=(1, ),\n",
    "        default_value=0,\n",
    "    ),\n",
    "    'episode_index_backwards': Field(\n",
    "        name='episode_index_backwards',\n",
    "        dtype='uint64',\n",
    "        shape=(1, ),\n",
    "        default_value=0,\n",
    "    ),\n",
    "}\n",
    "\n",
    "class ReplayPool(object):\n",
    "    \"\"\"A class used to save and replay data.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def add_sample(self, sample):\n",
    "        \"\"\"Add a transition tuple.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def terminate_episode(self):\n",
    "        \"\"\"Clean up pool after episode termination.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def size(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def add_path(self, path):\n",
    "        \"\"\"Add a rollout to the replay pool.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def random_batch(self, batch_size):\n",
    "        \"\"\"Return a random batch of size `batch_size`.\"\"\"\n",
    "        pass\n",
    "\n",
    "class FlexibleReplayPool(ReplayPool):\n",
    "    def __init__(self, max_size, fields):\n",
    "        super(FlexibleReplayPool, self).__init__()\n",
    "\n",
    "        max_size = int(max_size)\n",
    "        self._max_size = max_size\n",
    "\n",
    "        self.fields = {**fields, **INDEX_FIELDS}\n",
    "        # print('fields:{}'.format(fields))\n",
    "        self.data = tree.map_structure(self._initialize_field, self.fields)\n",
    "\n",
    "        self._pointer = 0\n",
    "        self._size = 0\n",
    "        self._samples_since_save = 0\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._size\n",
    "\n",
    "    def _initialize_field(self, field):\n",
    "        # 这里的关键是field.shape，指的是要存储数据的维度\n",
    "        field_shape = (self._max_size, *field.shape)\n",
    "        # print('field.name:{},field_shape:{}'.format(field.name,field_shape))\n",
    "        # np.zeros()\n",
    "        field_values = field.initializer(\n",
    "            field_shape, dtype=field.dtype)\n",
    "\n",
    "        return field_values\n",
    "\n",
    "    def _advance(self, count=1):\n",
    "        \"\"\"Handles bookkeeping after adding samples to the pool.\n",
    "\n",
    "        * Moves the pointer (`self._pointer`)\n",
    "        * Updates the size (`self._size`)\n",
    "        * Fixes the `episode_index_backwards` field, which might have become\n",
    "          out of date when the pool is full and we start overriding old\n",
    "          samples.\n",
    "        \"\"\"\n",
    "        self._pointer = (self._pointer + count) % self._max_size\n",
    "        self._size = min(self._size + count, self._max_size)\n",
    "\n",
    "        if self.data['episode_index_forwards'][self._pointer] != 0:\n",
    "            episode_tail_length = int(self.data[\n",
    "                                          'episode_index_backwards'\n",
    "                                      ][self._pointer, 0] + 1)\n",
    "            self.data[\n",
    "                'episode_index_forwards'\n",
    "            ][np.arange(\n",
    "                self._pointer, self._pointer + episode_tail_length\n",
    "            ) % self._max_size] = np.arange(episode_tail_length)[..., None]\n",
    "\n",
    "        self._samples_since_save += count\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        samples = tree.map_structure(lambda x: x[..., np.newaxis], sample)\n",
    "        self.add_samples(samples)\n",
    "\n",
    "    def add_samples(self, samples):\n",
    "        num_samples = tree.flatten(samples)[0].shape[0]\n",
    "\n",
    "        assert (('episode_index_forwards' in samples.keys())\n",
    "                is ('episode_index_backwards' in samples.keys()))\n",
    "        if 'episode_index_forwards' not in samples.keys():\n",
    "            samples['episode_index_forwards'] = np.full(\n",
    "                (num_samples, *self.fields['episode_index_forwards'].shape),\n",
    "                self.fields['episode_index_forwards'].default_value,\n",
    "                dtype=self.fields['episode_index_forwards'].dtype)\n",
    "            samples['episode_index_backwards'] = np.full(\n",
    "                (num_samples, *self.fields['episode_index_backwards'].shape),\n",
    "                self.fields['episode_index_backwards'].default_value,\n",
    "                dtype=self.fields['episode_index_backwards'].dtype)\n",
    "\n",
    "        index = np.arange(\n",
    "            self._pointer, self._pointer + num_samples) % self._max_size\n",
    "\n",
    "        def add_sample(path, data, new_values, field):\n",
    "            assert new_values.shape[0] == num_samples, (\n",
    "                new_values.shape, num_samples)\n",
    "            data[index] = new_values\n",
    "\n",
    "        tree.map_structure_with_path(\n",
    "            add_sample, self.data, samples, self.fields)\n",
    "\n",
    "        self._advance(num_samples)\n",
    "\n",
    "    def add_path(self, path):\n",
    "        # 给数据添加了 'episode_index_forwards'和'episode_index_backwards'字段，一个是【0，1，2，path_length】，另一个是倒序\n",
    "        path = path.copy()\n",
    "        path_length = tree.flatten(path)[0].shape[0]\n",
    "        path.update({\n",
    "            'episode_index_forwards': np.arange(\n",
    "                path_length,\n",
    "                dtype=self.fields['episode_index_forwards'].dtype\n",
    "            )[..., np.newaxis],\n",
    "            'episode_index_backwards': np.arange(\n",
    "                path_length,\n",
    "                dtype=self.fields['episode_index_backwards'].dtype\n",
    "            )[::-1, np.newaxis],\n",
    "        })\n",
    "\n",
    "        return self.add_samples(path)\n",
    "\n",
    "    def random_indices(self, batch_size):\n",
    "        if self._size == 0: return np.arange(0, 0)\n",
    "        return np.random.randint(0, self._size, batch_size)\n",
    "\n",
    "    def random_batch(self, batch_size, field_name_filter=None, **kwargs):\n",
    "        random_indices = self.random_indices(batch_size)\n",
    "        return self.batch_by_indices(\n",
    "            random_indices, field_name_filter=field_name_filter, **kwargs)\n",
    "\n",
    "    def random_sequence_batch(self, batch_size, **kwargs):\n",
    "        random_indices = self.random_indices(batch_size)\n",
    "        return self.sequence_batch_by_indices(random_indices, **kwargs)\n",
    "\n",
    "    def last_n_batch(self, last_n, field_name_filter=None, **kwargs):\n",
    "        last_n_indices = np.arange(\n",
    "            self._pointer - min(self.size, int(last_n)), self._pointer,\n",
    "            dtype=int\n",
    "        ) % self._max_size\n",
    "\n",
    "        return self.batch_by_indices(\n",
    "            last_n_indices, field_name_filter=field_name_filter, **kwargs)\n",
    "\n",
    "    def last_n_sequence_batch(self, last_n, **kwargs):\n",
    "        last_n_indices = np.arange(\n",
    "            self._pointer - min(self.size, int(last_n)), self._pointer,\n",
    "            dtype=int\n",
    "        ) % self._max_size\n",
    "\n",
    "        return self.sequence_batch_by_indices(last_n_indices, **kwargs)\n",
    "\n",
    "    def filter_fields(self, field_names, field_name_filter):\n",
    "        if isinstance(field_name_filter, str):\n",
    "            field_name_filter = [field_name_filter]\n",
    "\n",
    "        if isinstance(field_name_filter, (list, tuple)):\n",
    "            field_name_list = field_name_filter\n",
    "\n",
    "            def filter_fn(field_name):\n",
    "                return field_name in field_name_list\n",
    "\n",
    "        else:\n",
    "            filter_fn = field_name_filter\n",
    "\n",
    "        filtered_field_names = [\n",
    "            field_name for field_name in field_names\n",
    "            if filter_fn(field_name)\n",
    "        ]\n",
    "\n",
    "        return filtered_field_names\n",
    "\n",
    "    def batch_by_indices(self,\n",
    "                         indices,\n",
    "                         field_name_filter=None,\n",
    "                         validate_index=True):\n",
    "        if validate_index and np.any(self.size <= indices % self._max_size):\n",
    "            raise ValueError(\n",
    "                \"Tried to retrieve batch with indices greater than current\"\n",
    "                \" size\")\n",
    "\n",
    "        if field_name_filter is not None:\n",
    "            raise NotImplementedError(\"TODO(hartikainen)\")\n",
    "\n",
    "        batch = tree.map_structure(\n",
    "            lambda field: field[indices % self._max_size], self.data)\n",
    "        return batch\n",
    "\n",
    "    def sequence_batch_by_indices(self,\n",
    "                                  indices,\n",
    "                                  sequence_length,\n",
    "                                  field_name_filter=None):\n",
    "        if np.any(self.size <= indices % self._max_size):\n",
    "            raise ValueError(\n",
    "                \"Tried to retrieve batch with indices greater than current\"\n",
    "                \" size\")\n",
    "        if indices.size < 1:\n",
    "            return self.batch_by_indices(indices)\n",
    "\n",
    "        sequence_indices = (\n",
    "                indices[:, None] + np.arange(sequence_length)[None])\n",
    "        sequence_batch = self.batch_by_indices(\n",
    "            sequence_indices, validate_index=False)\n",
    "\n",
    "        if 'mask' in sequence_batch:\n",
    "            raise ValueError(\n",
    "                \"sequence_batch_by_indices adds a field 'mask' into the batch.\"\n",
    "                \" There already exists a 'mask' field in the batch. Please\"\n",
    "                \" remove it before using sequence_batch. TODO(hartikainen):\"\n",
    "                \" Allow mask name to be configured.\")\n",
    "\n",
    "        forward_diffs_0 = np.diff(\n",
    "            sequence_batch['episode_index_forwards'].astype(np.int64), axis=1)\n",
    "        forward_diffs_1 = np.pad(\n",
    "            forward_diffs_0, ([0, 0], [0, 1], [0, 0]),\n",
    "            mode='constant',\n",
    "            constant_values=-1)\n",
    "        cut_and_pad_sample_indices = (\n",
    "                np.argmax(forward_diffs_1[:, ::1, :] < 1, axis=1)\n",
    "                + 1)[..., 0]\n",
    "\n",
    "        sequence_batch['mask'] = np.where(\n",
    "            np.arange(sequence_length)[None, ...]\n",
    "            < cut_and_pad_sample_indices[..., None],\n",
    "            True,\n",
    "            False)\n",
    "\n",
    "        return sequence_batch\n",
    "\n",
    "    def save_latest_experience(self, pickle_path):\n",
    "        latest_samples = self.last_n_batch(self._samples_since_save)\n",
    "\n",
    "        with gzip.open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(latest_samples, f)\n",
    "\n",
    "        self._samples_since_save = 0\n",
    "\n",
    "    def load_experience(self, experience_path):\n",
    "        with gzip.open(experience_path, 'rb') as f:\n",
    "            latest_samples = pickle.load(f)\n",
    "\n",
    "        num_samples = tree.flatten(latest_samples)[0].shape[0]\n",
    "\n",
    "        def assert_shape(data):\n",
    "            assert data.shape[0] == num_samples, data.shape\n",
    "\n",
    "        tree.map_structure(assert_shape, latest_samples)\n",
    "\n",
    "        self.add_samples(latest_samples)\n",
    "        self._samples_since_save = 0\n",
    "\n",
    "# 这里要指定初始observation 和action的格式\n",
    "class SimpleReplayPool(FlexibleReplayPool):\n",
    "    def __init__(self,\n",
    "                 environment,\n",
    "                 *args,\n",
    "                 extra_fields=None,\n",
    "                 **kwargs):\n",
    "        extra_fields = extra_fields or {}\n",
    "        ################## 重新指定observation_space #################\n",
    "        obs = environment.get_obs()\n",
    "        # 处理后的observation\n",
    "        observation_space = np.concatenate(([obs.rho.max()],[obs.rho.min()]))\n",
    "        # 动作编号\n",
    "        action_space = np.array(0)\n",
    "\n",
    "        ############################################################\n",
    "        # observation_space = environment.observation_space\n",
    "        # action_space = environment.action_space\n",
    "\n",
    "        self._environment = environment\n",
    "        self._observation_space = observation_space\n",
    "        self._action_space = action_space\n",
    "\n",
    "        fields = {\n",
    "            'observations':Field(\n",
    "                name='observations',\n",
    "                dtype=observation_space.dtype,\n",
    "                shape=observation_space.shape),\n",
    "            'next_observations':Field(\n",
    "                name='next_observations',\n",
    "                dtype=observation_space.dtype,\n",
    "                shape=observation_space.shape),\n",
    "            'actions': Field(\n",
    "                name='actions',\n",
    "                dtype=action_space.dtype,\n",
    "                shape=action_space.shape),\n",
    "            'rewards': Field(\n",
    "                name='rewards',\n",
    "                dtype='float32',\n",
    "                shape=(1, )),\n",
    "            # terminals[i] = a terminal was received at time i\n",
    "            'terminals': Field(\n",
    "                name='terminals',\n",
    "                dtype='bool',\n",
    "                shape=(1, )),\n",
    "            **extra_fields\n",
    "        }\n",
    "\n",
    "        super(SimpleReplayPool, self).__init__(\n",
    "            *args, fields=fields, **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Trajectory 采集器\n",
    "\n",
    "class BaseSampler(object):\n",
    "    def __init__(self,\n",
    "                 max_path_length,\n",
    "                 environment=None,\n",
    "                 policy=None,\n",
    "                 pool=None,\n",
    "                 store_last_n_paths=10):\n",
    "        self._max_path_length = max_path_length\n",
    "        self._store_last_n_paths = store_last_n_paths\n",
    "        self._last_n_paths = deque(maxlen=store_last_n_paths)\n",
    "\n",
    "        self.environment = environment\n",
    "        self.policy = policy\n",
    "        self.pool = pool\n",
    "\n",
    "    def initialize(self, environment, policy, pool):\n",
    "        self.environment = environment\n",
    "        self.policy = policy\n",
    "        self.pool = pool\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    def clear_last_n_paths(self):\n",
    "        self._last_n_paths.clear()\n",
    "\n",
    "    def get_last_n_paths(self, n=None):\n",
    "        if n is None:\n",
    "            n = self._store_last_n_paths\n",
    "\n",
    "        last_n_paths = tuple(islice(self._last_n_paths, None, n))\n",
    "\n",
    "        return last_n_paths\n",
    "\n",
    "    def sample(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def terminate(self):\n",
    "        self.environment.close()\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        diagnostics = OrderedDict({'pool-size': self.pool.size})\n",
    "        return diagnostics\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = {\n",
    "            key: value for key, value in self.__dict__.items()\n",
    "            if key not in (\n",
    "                'environment',\n",
    "                'policy',\n",
    "                'pool',\n",
    "                '_last_n_paths',\n",
    "                '_current_observation',\n",
    "                '_current_path',\n",
    "                '_is_first_step',\n",
    "            )\n",
    "        }\n",
    "\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "        self.environment = None\n",
    "        self.policy = None\n",
    "        self.pool = None\n",
    "        # TODO(hartikainen): Maybe try restoring these from the pool?\n",
    "        self._last_n_paths = deque(maxlen=self._store_last_n_paths)\n",
    "\n",
    "# 使用时要处理下observation\n",
    "class SimpleSampler(BaseSampler):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SimpleSampler, self).__init__(**kwargs)\n",
    "\n",
    "        self._last_path_return = 0\n",
    "        self._max_path_return = -np.inf\n",
    "        self._n_episodes = 0\n",
    "        self._total_samples = 0\n",
    "\n",
    "        self._is_first_step = True\n",
    "\n",
    "    def reset(self):\n",
    "        if self.policy is not None:\n",
    "            self.policy.reset()\n",
    "\n",
    "        self._path_length = 0\n",
    "        self._path_return = 0\n",
    "        self._current_path = []\n",
    "        #################### 修改observation #####################\n",
    "        obs = self.environment.reset()\n",
    "        self._current_observation = np.concatenate(([obs.rho.max()],[obs.rho.min()]))\n",
    "        #########################################\n",
    "        # self._current_observation = self.environment.reset()\n",
    "\n",
    "    @property\n",
    "    def _policy_input(self):\n",
    "        return self._current_observation\n",
    "\n",
    "    def _process_sample(self,\n",
    "                        observation,\n",
    "                        action,\n",
    "                        reward,\n",
    "                        terminal,\n",
    "                        next_observation):\n",
    "        processed_observation = {\n",
    "            'observations': observation,\n",
    "            'actions': action,\n",
    "            'rewards': np.atleast_1d(reward),\n",
    "            'terminals': np.atleast_1d(terminal),\n",
    "            'next_observations': next_observation,\n",
    "        }\n",
    "\n",
    "        return processed_observation\n",
    "\n",
    "    def sample(self):\n",
    "        if self._is_first_step:\n",
    "            self.reset()\n",
    "\n",
    "        # action = self.policy.action(self._policy_input).numpy()\n",
    "        ##########################  处理action #############################\n",
    "        act = self.policy.action(self.environment)\n",
    "        action = 0\n",
    "        #######################################################\n",
    "\n",
    "        next_obs, reward, terminal, info = self.environment.step(\n",
    "            act)\n",
    "        # print(\"next_obs:{},terminal:{}\".format(next_obs,terminal))\n",
    "        next_observation=np.concatenate(([next_obs.rho.max()],[next_obs.rho.min()]))\n",
    "        self._path_length += 1\n",
    "        self._path_return += reward\n",
    "        self._total_samples += 1\n",
    "\n",
    "        processed_sample = self._process_sample(\n",
    "            observation=self._current_observation,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            terminal=terminal,\n",
    "            next_observation=next_observation,\n",
    "        )\n",
    "        print(\"processed_sample:{}\".format(processed_sample))\n",
    "\n",
    "        self._current_path.append(processed_sample)\n",
    "\n",
    "        if terminal or self._path_length >= self._max_path_length:\n",
    "            print(\"self._current_path:{}\".format(self._current_path))\n",
    "            last_path = tree.map_structure(\n",
    "                lambda *x: np.stack(x, axis=0), *self._current_path)\n",
    "\n",
    "            self.pool.add_path({\n",
    "                key: value\n",
    "                for key, value in last_path.items()\n",
    "                if key != 'infos'\n",
    "            })\n",
    "\n",
    "            self._last_n_paths.appendleft(last_path)\n",
    "\n",
    "            self._max_path_return = max(self._max_path_return,\n",
    "                                        self._path_return)\n",
    "            self._last_path_return = self._path_return\n",
    "            self._n_episodes += 1\n",
    "\n",
    "            self.pool.terminate_episode()\n",
    "\n",
    "            self._is_first_step = True\n",
    "            # Reset is done in the beginning of next episode, see above.\n",
    "\n",
    "        else:\n",
    "            self._current_observation = next_observation\n",
    "            self._is_first_step = False\n",
    "\n",
    "        return next_observation, reward, terminal, info\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        diagnostics = super(SimpleSampler, self).get_diagnostics()\n",
    "        diagnostics.update({\n",
    "            'max-path-return': self._max_path_return,\n",
    "            'last-path-return': self._last_path_return,\n",
    "            'episodes': self._n_episodes,\n",
    "            'total-samples': self._total_samples,\n",
    "        })\n",
    "\n",
    "        return diagnostics\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class agent_policy():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def action(self,env):\n",
    "        return env.action_space({})\n",
    "    def reset(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 环境加载\n",
    "import grid2op\n",
    "from grid2op.Reward import BaseReward, RedispReward, L2RPNSandBoxScore\n",
    "import numpy as np\n",
    "from grid2op.Parameters import Parameters\n",
    "\n",
    "other_rewards = {}\n",
    "other_rewards[\"tmp_score_codalab\"] = L2RPNSandBoxScore\n",
    "input_dir = '../input_data_local'\n",
    "parameters = Parameters()\n",
    "parameters.HARD_OVERFLOW_THRESHOLD = 3.0\n",
    "parameters.MAX_SUB_CHANGED = 6\n",
    "parameters.NB_TIMESTEP_OVERFLOW_ALLOWED = 4\n",
    "parameters.MAX_LINE_STATUS_CHANGED = 100\n",
    "\n",
    "env = grid2op.make(input_dir, param=parameters,\n",
    "                   reward_class=RedispReward,\n",
    "                   other_rewards=other_rewards)\n",
    "env.seed(10)\n",
    "env.set_id(0)\n",
    "obs = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "path_length = 4\n",
    "pool = SimpleReplayPool(environment=env, max_size=path_length)\n",
    "agent = agent_policy()\n",
    "sampler = SimpleSampler(\n",
    "    environment=env,\n",
    "    policy=agent,\n",
    "    pool=pool,\n",
    "    max_path_length=path_length)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sampler.pool.size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_sample:{'observations': array([0.6382429 , 0.01134157], dtype=float32), 'actions': 0, 'rewards': array([937.0288], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.6378086 , 0.00742103], dtype=float32)}\n",
      "processed_sample:{'observations': array([0.6378086 , 0.00742103], dtype=float32), 'actions': 0, 'rewards': array([938.5002], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.63766575, 0.01766751], dtype=float32)}\n",
      "processed_sample:{'observations': array([0.63766575, 0.01766751], dtype=float32), 'actions': 0, 'rewards': array([938.37775], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.6369229 , 0.01489069], dtype=float32)}\n",
      "processed_sample:{'observations': array([0.6369229 , 0.01489069], dtype=float32), 'actions': 0, 'rewards': array([939.53784], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.63768613, 0.01580404], dtype=float32)}\n",
      "self._current_path:[{'observations': array([0.6382429 , 0.01134157], dtype=float32), 'actions': 0, 'rewards': array([937.0288], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.6378086 , 0.00742103], dtype=float32)}, {'observations': array([0.6378086 , 0.00742103], dtype=float32), 'actions': 0, 'rewards': array([938.5002], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.63766575, 0.01766751], dtype=float32)}, {'observations': array([0.63766575, 0.01766751], dtype=float32), 'actions': 0, 'rewards': array([938.37775], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.6369229 , 0.01489069], dtype=float32)}, {'observations': array([0.6369229 , 0.01489069], dtype=float32), 'actions': 0, 'rewards': array([939.53784], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.63768613, 0.01580404], dtype=float32)}]\n",
      "processed_sample:{'observations': array([0.64314294, 0.00418756], dtype=float32), 'actions': 0, 'rewards': array([1037.6371], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.63999593, 0.00563635], dtype=float32)}\n",
      "processed_sample:{'observations': array([0.63999593, 0.00563635], dtype=float32), 'actions': 0, 'rewards': array([1042.6559], dtype=float32), 'terminals': array([False]), 'next_observations': array([0.64282143, 0.01352443], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    sampler.sample()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{'observations': array([[0.6382429 , 0.01134157],\n",
      "       [0.6378086 , 0.00742103],\n",
      "       [0.63766575, 0.01766751],\n",
      "       [0.6369229 , 0.01489069]], dtype=float32), 'next_observations': array([[0.6378086 , 0.00742103],\n",
      "       [0.63766575, 0.01766751],\n",
      "       [0.6369229 , 0.01489069],\n",
      "       [0.63768613, 0.01580404]], dtype=float32), 'actions': array([0, 0, 0, 0]), 'rewards': array([[937.0288 ],\n",
      "       [938.5002 ],\n",
      "       [938.37775],\n",
      "       [939.53784]], dtype=float32), 'terminals': array([[False],\n",
      "       [False],\n",
      "       [False],\n",
      "       [False]]), 'episode_index_forwards': array([[0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [3]], dtype=uint64), 'episode_index_backwards': array([[3],\n",
      "       [2],\n",
      "       [1],\n",
      "       [0]], dtype=uint64)}\n"
     ]
    }
   ],
   "source": [
    "print(sampler.pool.size)\n",
    "print(sampler.pool.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def rollout(environment,policy,path_length,replay_pool_class=SimpleReplayPool,sampler_class=SimpleSampler,\n",
    "            break_on_terminal=True):\n",
    "    pool = replay_pool_class(environment, max_size=path_length)\n",
    "    sampler = sampler_class(\n",
    "        environment=environment,\n",
    "        policy=policy,\n",
    "        pool=pool,\n",
    "        max_path_length=path_length)\n",
    "\n",
    "    infos = defaultdict(list)\n",
    "\n",
    "    t = 0\n",
    "    for t in range(path_length):\n",
    "        observation, reward, terminal, info = sampler.sample()\n",
    "        for key, value in info.items():\n",
    "            infos[key].append(value)\n",
    "\n",
    "        if terminal:\n",
    "            policy.reset()\n",
    "            if break_on_terminal: break\n",
    "\n",
    "    assert pool._size == t + 1\n",
    "\n",
    "    path = pool.batch_by_indices(np.arange(pool._size))\n",
    "    path['infos'] = infos\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def rollouts(n_paths, *args, **kwargs):\n",
    "    paths = [rollout(*args, **kwargs) for i in range(n_paths)]\n",
    "    return paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations:[[0.6382429  0.01134157]\n",
      " [0.6378086  0.00742103]\n",
      " [0.63766575 0.01766751]\n",
      " [0.6369229  0.01489069]]\n",
      "actions:[0 0 0 0]\n",
      "observation:[0.6382429  0.01134157],action:[0]\n",
      "[[0.6382429  0.01134157 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "observations = sampler.pool.data['observations']\n",
    "print(\"observations:{}\".format(observations))\n",
    "actions = sampler.pool.data['actions']\n",
    "print(\"actions:{}\".format(actions))\n",
    "observation = observations[0]\n",
    "action = [actions[0]]\n",
    "print(\"observation:{},action:{}\".format(observation,action))\n",
    "Input_Demo = np.concatenate((observation,action),axis=-1)\n",
    "Input_Demo = Input_Demo.reshape((1,len(Input_Demo)))\n",
    "print(Input_Demo)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_sizes:(50, 50),output_shape:[1],output_size:1\n",
      "hidden_layer_sizes:(50, 50),output_shape:[1],output_size:1\n"
     ]
    }
   ],
   "source": [
    "from q_value_nwtwork import double_feedforward_Q_function\n",
    "Qs = double_feedforward_Q_function(input_demo=Input_Demo,hidden_layer_sizes=(50,50),name = 'Double_Q_Value_Function')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:[[0.6382429  0.01134157 0.        ]\n",
      " [0.63780862 0.00742103 0.        ]\n",
      " [0.63766575 0.01766751 0.        ]\n",
      " [0.6369229  0.01489069 0.        ]]\n",
      "Q.values(observation,action):[[-0.05132855]\n",
      " [-0.05176813]\n",
      " [-0.05051375]\n",
      " [-0.05078914]]\n",
      "inputs:[[0.6382429  0.01134157 0.        ]\n",
      " [0.63780862 0.00742103 0.        ]\n",
      " [0.63766575 0.01766751 0.        ]\n",
      " [0.6369229  0.01489069 0.        ]]\n",
      "Q.values(observation,action):[[0.03838278]\n",
      " [0.03934149]\n",
      " [0.03675331]\n",
      " [0.03740422]]\n"
     ]
    }
   ],
   "source": [
    "for Q in Qs:\n",
    "    print(\"Q.values(observation,action):{}\".format(Q.values(observations,actions)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_sizes:(50, 50),output_shape:(360,),output_size:360\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GaussianPolicy.actions of <tensorflow.python.eager.function.TfMethodTarget object at 0x00000284E7406F88>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GaussianPolicy.actions of <tensorflow.python.eager.function.TfMethodTarget object at 0x00000284E7406F88>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "tf.Tensor([198], shape=(1,), dtype=int64)\n",
      "tf.Tensor([198 198 198 198], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "from discret_policy import GaussianPolicy\n",
    "Action_Num = 360\n",
    "Input_Demo = observation.reshape((1,len(observation)))\n",
    "M = 50\n",
    "policy = GaussianPolicy(input_demo=Input_Demo,output_shape=Action_Num,**{'hidden_layer_sizes':(M, M),'observation_keys': None,'preprocessors': None,})\n",
    "print(policy.actions(Input_Demo))\n",
    "print(policy.actions(observations))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}