{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ray\n",
    "from ray import tune\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tree\n",
    "import copy\n",
    "import grid2op\n",
    "from grid2op.Reward import BaseReward, RedispReward, L2RPNSandBoxScore\n",
    "import numpy as np\n",
    "from grid2op.Parameters import Parameters\n",
    "from Sampler import SimpleReplayPool,SimpleSampler\n",
    "from ExperimentRunner import ExperimentRunner\n",
    "from variant_spec import get_variant_spec\n",
    "from get_parser import get_parser\n",
    "from experiment_kwargs import generate_experiment_kwargs\n",
    "from q_value_nwtwork import double_feedforward_Q_function\n",
    "from discret_policy import GaussianPolicy\n",
    "from SAC_Algorithm import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def run_example_local(example_module_name, example_argv, local_mode=False):\n",
    "    \"\"\"Run example locally, potentially parallelizing across cpus/gpus.\"\"\"\n",
    "    # example_module_name:'examples.development'; --algorithm SAC  --universe gym  --domain HalfCheetah\n",
    "    # --task v3  --exp-name my-sac-experiment-1 --checkpoint-frequency 1000  # Save the checkpoint to resume training later\n",
    "    # 添加了argument parser, 并解析了相关参数\n",
    "    example_args = get_parser().parse_args(example_argv)\n",
    "    variant_spec = get_variant_spec(example_args)\n",
    "    trainable_class = ExperimentRunner\n",
    "\n",
    "    experiment_kwargs = generate_experiment_kwargs(variant_spec, example_args)\n",
    "\n",
    "\n",
    "    ray.init(\n",
    "        num_cpus=example_args.cpus,\n",
    "        num_gpus=example_args.gpus,\n",
    "        resources=example_args.resources or {},\n",
    "        local_mode=local_mode,\n",
    "        include_dashboard=example_args.include_dashboard,\n",
    "        _temp_dir=example_args.temp_dir)\n",
    "\n",
    "    tune.run(\n",
    "        trainable_class,\n",
    "        **experiment_kwargs,\n",
    "        server_port=example_args.server_port,\n",
    "        fail_fast=example_args.fail_fast,\n",
    "        scheduler=None,\n",
    "        reuse_actors=True)\n",
    "\n",
    "def get_environment_from_params():\n",
    "    other_rewards = {}\n",
    "    other_rewards[\"tmp_score_codalab\"] = L2RPNSandBoxScore\n",
    "    input_dir = '../input_data_local'\n",
    "    parameters = Parameters()\n",
    "    parameters.HARD_OVERFLOW_THRESHOLD = 3.0\n",
    "    parameters.MAX_SUB_CHANGED = 6\n",
    "    parameters.NB_TIMESTEP_OVERFLOW_ALLOWED = 4\n",
    "    parameters.MAX_LINE_STATUS_CHANGED = 100\n",
    "\n",
    "    env = grid2op.make(input_dir, param=parameters,\n",
    "                       reward_class=RedispReward,\n",
    "                       other_rewards=other_rewards)\n",
    "    env.seed(10)\n",
    "    env.set_id(0)\n",
    "    obs = env.reset()\n",
    "\n",
    "    return env\n",
    "\n",
    "def run_grid2op(example_argv):\n",
    "    Hidden_Layer_Sizes = (100,100)\n",
    "    example_args = get_parser().parse_args(example_argv)\n",
    "    variant_spec = get_variant_spec(example_args)\n",
    "    variant = generate_experiment_kwargs(variant_spec, example_args)\n",
    "    print(\"variant:{}\".format(variant))\n",
    "    print(\"variant['config']:{}\".format(variant['config']))\n",
    "\n",
    "    environment_params = variant['config']['environment_params']\n",
    "    training_environment = get_environment_from_params()\n",
    "    evaluation_environment = get_environment_from_params()\n",
    "\n",
    "    obs = training_environment.get_obs()\n",
    "    observation = np.concatenate(([obs.rho.max()],[obs.rho.min()]))\n",
    "    action = [0]\n",
    "    print(\"observation:{},action:{}\".format(observation,action))\n",
    "    Input_Demo_ = np.concatenate((observation,action))\n",
    "    Input_Demo = Input_Demo_.reshape((1,len(Input_Demo_)))\n",
    "\n",
    "    Qs = double_feedforward_Q_function(input_demo=Input_Demo,\n",
    "                                       hidden_layer_sizes=Hidden_Layer_Sizes,\n",
    "                                       name = 'Double_Q_Value_Function')\n",
    "\n",
    "    Qs_Target = double_feedforward_Q_function(input_demo=Input_Demo,\n",
    "                                              hidden_layer_sizes=Hidden_Layer_Sizes,\n",
    "                                              name = 'Target_Double_Q_Value_Function')\n",
    "\n",
    "\n",
    "\n",
    "    Action_Num = len(np.load('../Tutor/Single_Sub_Structure.npy',allow_pickle=True))\\\n",
    "                 + len(np.load('../Tutor/Multiple_Sub_Structure_Last.npy',allow_pickle=True))+ 3\n",
    "    Input_Demo = observation.reshape((1,len(observation)))\n",
    "    policy = GaussianPolicy(input_demo=Input_Demo,output_shape=Action_Num,\n",
    "                            hidden_layer_sizes=Hidden_Layer_Sizes)\n",
    "\n",
    "    # 这个需要重新指定,path_length 和 环境\n",
    "\n",
    "    replay_pool = SimpleReplayPool(environment=training_environment, max_size=10000)\n",
    "\n",
    "    path_length = 4\n",
    "    sampler = SimpleSampler(\n",
    "        environment=training_environment,\n",
    "        policy=policy,\n",
    "        pool=replay_pool,\n",
    "        max_path_length=path_length)\n",
    "\n",
    "    for i in range(5):\n",
    "        sampler.sample()\n",
    "\n",
    "    # print('replay_pool.data:{}'.format(replay_pool.data))\n",
    "\n",
    "    algorithm = SAC(training_environment = training_environment,\n",
    "                    evaluation_environment = evaluation_environment,\n",
    "                    policy = policy,\n",
    "                    Qs = Qs,\n",
    "                    Qs_Target=Qs_Target,\n",
    "                    Discret_Action_Num = Action_Num,\n",
    "                    sampler = sampler,\n",
    "                    pool = replay_pool\n",
    "                    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gitpython not installed. Unable to log git rev. Run `pip install gitpython` if you want git revs to be logged.\n",
      "variant:{'name': '2021-11-17T20-35-36-2021-11-17T20-35-36', 'resources_per_trial': {'cpu': 12}, 'config': {'git_sha': None, 'environment_params': {'training': {'domain': 'l2rpn_icaps_2021', 'task': 'chronics', 'universe': 'grid2op', 'kwargs': {}}, 'evaluation': <ray.tune.sample.Function object at 0x0000026657C376C8>}, 'policy_params': {'class_name': 'FeedforwardGaussianPolicy', 'config': {'hidden_layer_sizes': (256, 256), 'squash': True, 'observation_keys': None, 'preprocessors': None}}, 'exploration_policy_params': {'class_name': 'ContinuousUniformPolicy', 'config': {'observation_keys': <ray.tune.sample.Function object at 0x00000266578C3D08>}}, 'Q_params': {'class_name': 'double_feedforward_Q_function', 'config': {'hidden_layer_sizes': (256, 256), 'observation_keys': None, 'preprocessors': None}}, 'algorithm_params': {'config': {'train_every_n_steps': 1, 'n_train_repeat': 1, 'eval_render_kwargs': {}, 'eval_n_episodes': 1, 'num_warmup_samples': <ray.tune.sample.Function object at 0x0000026657BFF688>, 'n_epochs': 24, 'epoch_length': 2016, 'min_pool_size': 10000, 'batch_size': 256}}, 'replay_pool_params': {'class_name': 'SimpleReplayPool', 'config': {'max_size': <ray.tune.sample.Function object at 0x0000026657104CC8>}}, 'sampler_params': {'class_name': 'SimpleSampler', 'config': {'max_path_length': 2016}}, 'run_params': {'host_name': 'DESKTOP-12PQ151', 'seed': <ray.tune.sample.Function object at 0x0000026657C23EC8>, 'checkpoint_at_end': True, 'checkpoint_frequency': <ray.tune.sample.Function object at 0x0000026657C23D08>, 'checkpoint_replay_pool': False, 'run_eagerly': None}, 'restore': None}, 'local_dir': '~/ray_results\\\\grid2op\\\\l2rpn_icaps_2021\\\\chronics', 'num_samples': 1, 'upload_dir': None, 'checkpoint_freq': <ray.tune.sample.Function object at 0x0000026657C23D08>, 'checkpoint_at_end': True, 'max_failures': 3, 'trial_name_creator': <function generate_experiment_kwargs.<locals>.create_trial_name_creator.<locals>.trial_name_creator at 0x0000026657C155E8>, 'restore': None}\n",
      "variant['config']:{'git_sha': None, 'environment_params': {'training': {'domain': 'l2rpn_icaps_2021', 'task': 'chronics', 'universe': 'grid2op', 'kwargs': {}}, 'evaluation': <ray.tune.sample.Function object at 0x0000026657C376C8>}, 'policy_params': {'class_name': 'FeedforwardGaussianPolicy', 'config': {'hidden_layer_sizes': (256, 256), 'squash': True, 'observation_keys': None, 'preprocessors': None}}, 'exploration_policy_params': {'class_name': 'ContinuousUniformPolicy', 'config': {'observation_keys': <ray.tune.sample.Function object at 0x00000266578C3D08>}}, 'Q_params': {'class_name': 'double_feedforward_Q_function', 'config': {'hidden_layer_sizes': (256, 256), 'observation_keys': None, 'preprocessors': None}}, 'algorithm_params': {'config': {'train_every_n_steps': 1, 'n_train_repeat': 1, 'eval_render_kwargs': {}, 'eval_n_episodes': 1, 'num_warmup_samples': <ray.tune.sample.Function object at 0x0000026657BFF688>, 'n_epochs': 24, 'epoch_length': 2016, 'min_pool_size': 10000, 'batch_size': 256}}, 'replay_pool_params': {'class_name': 'SimpleReplayPool', 'config': {'max_size': <ray.tune.sample.Function object at 0x0000026657104CC8>}}, 'sampler_params': {'class_name': 'SimpleSampler', 'config': {'max_path_length': 2016}}, 'run_params': {'host_name': 'DESKTOP-12PQ151', 'seed': <ray.tune.sample.Function object at 0x0000026657C23EC8>, 'checkpoint_at_end': True, 'checkpoint_frequency': <ray.tune.sample.Function object at 0x0000026657C23D08>, 'checkpoint_replay_pool': False, 'run_eagerly': None}, 'restore': None}\n",
      "observation:[0.62659556 0.01190448],action:[0]\n",
      "hidden_layer_sizes:(100, 100),output_shape:[1],output_size:1\n",
      "hidden_layer_sizes:(100, 100),output_shape:[1],output_size:1\n",
      "hidden_layer_sizes:(100, 100),output_shape:[1],output_size:1\n",
      "hidden_layer_sizes:(100, 100),output_shape:[1],output_size:1\n",
      "hidden_layer_sizes:(100, 100),output_shape:(384,),output_size:384\n"
     ]
    }
   ],
   "source": [
    "example_argv = {}\n",
    "Hidden_Layer_Sizes = (100,100)\n",
    "example_args = get_parser().parse_args(example_argv)\n",
    "variant_spec = get_variant_spec(example_args)\n",
    "variant = generate_experiment_kwargs(variant_spec, example_args)\n",
    "print(\"variant:{}\".format(variant))\n",
    "print(\"variant['config']:{}\".format(variant['config']))\n",
    "environment_params = variant['config']['environment_params']\n",
    "training_environment = get_environment_from_params()\n",
    "evaluation_environment = get_environment_from_params()\n",
    "\n",
    "obs = training_environment.get_obs()\n",
    "observation = np.concatenate(([obs.rho.max()],[obs.rho.min()]))\n",
    "action = [0]\n",
    "print(\"observation:{},action:{}\".format(observation,action))\n",
    "Input_Demo_ = np.concatenate((observation,action))\n",
    "Input_Demo = Input_Demo_.reshape((1,len(Input_Demo_)))\n",
    "\n",
    "Qs = double_feedforward_Q_function(input_demo=Input_Demo,\n",
    "                                   hidden_layer_sizes=Hidden_Layer_Sizes,\n",
    "                                   name = 'Double_Q_Value_Function')\n",
    "\n",
    "Qs_Target = double_feedforward_Q_function(input_demo=Input_Demo,\n",
    "                                          hidden_layer_sizes=Hidden_Layer_Sizes,\n",
    "                                          name = 'Target_Double_Q_Value_Function')\n",
    "\n",
    "\n",
    "Action_Num = len(np.load('../Tutor/Single_Sub_Structure.npy',allow_pickle=True))\\\n",
    "             + len(np.load('../Tutor/Multiple_Sub_Structure_Last.npy',allow_pickle=True))+ 3\n",
    "Input_Demo = observation.reshape((1,len(observation)))\n",
    "policy = GaussianPolicy(input_demo=Input_Demo,output_shape=Action_Num,\n",
    "                        hidden_layer_sizes=Hidden_Layer_Sizes)\n",
    "\n",
    "# 这个需要重新指定,path_length 和 环境\n",
    "\n",
    "replay_pool = SimpleReplayPool(environment=training_environment, max_size=10000)\n",
    "\n",
    "path_length = 4\n",
    "sampler = SimpleSampler(\n",
    "    environment=training_environment,\n",
    "    policy=policy,\n",
    "    pool=replay_pool,\n",
    "    max_path_length=path_length)\n",
    "\n",
    "# for i in range(5):\n",
    "#     sampler.sample()\n",
    "\n",
    "# print('replay_pool.data:{}'.format(replay_pool.data))\n",
    "\n",
    "algorithm = SAC(training_environment = training_environment,\n",
    "                evaluation_environment = evaluation_environment,\n",
    "                policy = policy,\n",
    "                Qs = Qs,\n",
    "                Qs_Target=Qs_Target,\n",
    "                Discret_Action_Num = Action_Num,\n",
    "                sampler = sampler,\n",
    "                pool = replay_pool,\n",
    "                min_pool_size=20,\n",
    "                batch_size=5,\n",
    "                n_epochs=10,\n",
    "                epoch_length=2016,\n",
    "                train_every_n_steps=3,\n",
    "                n_train_repeat=3,\n",
    "                target_update_interval=100\n",
    "\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object RLAlgorithm._train at 0x0000026657BEEB48>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algorithm._update_target(0.9)\n",
    "# Qs = double_feedforward_Q_function(input_demo=Input_Demo,**variant['config']['Q_params']['config'])\n",
    "algorithm.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}