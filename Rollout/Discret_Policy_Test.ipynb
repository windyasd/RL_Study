{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tree\n",
    "import abc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError:\n",
    "    yaml = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    def __init__(self,\n",
    "                 input_demo,\n",
    "                 input_shapes,\n",
    "                 output_shape,\n",
    "                 observation_keys=None,\n",
    "                 preprocessors=None,\n",
    "                 name='policy'):\n",
    "        self._input_shapes = input_shapes\n",
    "        self._output_shape = output_shape\n",
    "        self._observation_keys = observation_keys\n",
    "        self.input_demo = input_demo\n",
    "\n",
    "        print(\"input_shapes:{}\".format(input_shapes))\n",
    "\n",
    "        self._name = name\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "\n",
    "    @property\n",
    "    def observation_keys(self):\n",
    "        return self._observation_keys\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset and clean the policy.\"\"\"\n",
    "\n",
    "    def get_weights(self):\n",
    "        return []\n",
    "\n",
    "    def set_weights(self, *args, **kwargs):\n",
    "        return []\n",
    "\n",
    "    def save_weights(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_weights(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        \"\"\"Returns the list of all policy variables/weights.\n",
    "\n",
    "        Returns:\n",
    "          A list of variables.\n",
    "        \"\"\"\n",
    "        return self.trainable_weights + self.non_trainable_weights\n",
    "\n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def non_trainable_weights(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        \"\"\"Returns the list of all policy variables/weights.\n",
    "\n",
    "        Alias of `self.weights`.\n",
    "\n",
    "        Returns:\n",
    "          A list of variables.\n",
    "        \"\"\"\n",
    "        return self.weights\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return self.trainable_weights\n",
    "\n",
    "    @property\n",
    "    def non_trainable_variables(self):\n",
    "        return self.non_trainable_weights\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def actions(self, inputs):\n",
    "        \"\"\"Compute actions for given inputs (e.g. observations).\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def action(self, *args, **kwargs):\n",
    "        \"\"\"Compute an action for a single input, (e.g. observation).\"\"\"\n",
    "        args_, kwargs_ = tree.map_structure(\n",
    "            lambda x: x[None, ...], (args, kwargs))\n",
    "        actions = self.actions(*args_, **kwargs_)\n",
    "        action = tree.map_structure(lambda x: x[0], actions)\n",
    "        return action\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_probs(self, inputs, actions):\n",
    "        \"\"\"Compute log probabilities for given actions.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def log_prob(self, *args, **kwargs):\n",
    "        \"\"\"Compute the log probability for a single action.\"\"\"\n",
    "        args_, kwargs_ = tree.map_structure(\n",
    "            lambda x: x[None, ...], (args, kwargs))\n",
    "        log_probs = self.log_probs(*args_, **kwargs_)\n",
    "        log_prob = tree.map_structure(lambda x: x[0], log_probs)\n",
    "        return log_prob\n",
    "\n",
    "    def actions_and_log_probs(self, *args, **kwargs):\n",
    "        \"\"\"Compute actions for given inputs (e.g. observations).\"\"\"\n",
    "        actions = self.actions(*args, **kwargs)\n",
    "        log_probs = self.log_probs(*args, **kwargs, actions=actions)\n",
    "        return actions, log_probs\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def probs(self, inputs, actions):\n",
    "        \"\"\"Compute probabilities for given actions.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prob(self, *args, **kwargs):\n",
    "        \"\"\"Compute the probability for a single action.\"\"\"\n",
    "        args_, kwargs_ = tree.map_structure(\n",
    "            lambda x: x[None, ...], (args, kwargs))\n",
    "        probs = self.probs(*args_, **kwargs_)\n",
    "        prob = tree.map_structure(lambda x: x[0], probs)\n",
    "        return prob\n",
    "\n",
    "    def _filter_observations(self, observations):\n",
    "        if (isinstance(observations, dict)\n",
    "                and self._observation_keys is not None):\n",
    "            observations = type(observations)((\n",
    "                (key, observations[key])\n",
    "                for key in self.observation_keys\n",
    "            ))\n",
    "        return observations\n",
    "\n",
    "    def get_diagnostics(self, inputs):\n",
    "        \"\"\"Return diagnostic information of the policy.\n",
    "\n",
    "        Arguments:\n",
    "            conditions: Observations to run the diagnostics for.\n",
    "        Returns:\n",
    "            diagnostics: OrderedDict of diagnostic information.\n",
    "        \"\"\"\n",
    "        diagnostics = OrderedDict()\n",
    "        return diagnostics\n",
    "\n",
    "    def get_diagnostics_np(self, *args, **kwargs):\n",
    "        diagnostics = self.get_diagnostics(*args, **kwargs)\n",
    "        diagnostics_np = tree.map_structure(lambda x: x.numpy(), diagnostics)\n",
    "        return diagnostics_np\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'input_shapes': self._input_shapes,\n",
    "            'output_shape': self._output_shape,\n",
    "            'observation_keys': self._observation_keys,\n",
    "            'name': self._name,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def _updated_config(self):\n",
    "        config = self.get_config()\n",
    "        model_config = {\n",
    "            'class_name': self.__class__.__name__,\n",
    "            'config': config,\n",
    "        }\n",
    "        return model_config\n",
    "\n",
    "    def to_yaml(self, **kwargs):\n",
    "        if yaml is None:\n",
    "            raise ImportError(\n",
    "                \"Requires yaml module installed (`pip install pyyaml`).\")\n",
    "\n",
    "        yaml.dump(self._updated_config(), **kwargs)\n",
    "\n",
    "    def to_json(self, **kwargs):\n",
    "        model_config = self._updated_config()\n",
    "        return json.dumps(model_config, **kwargs)\n",
    "\n",
    "    def save(self, filepath, overwrite=True):\n",
    "        assert overwrite\n",
    "        config_yaml = self.to_yaml()\n",
    "        with open(f\"{filepath}-config.json\", 'w') as f:\n",
    "            json.dump(config_yaml, f)\n",
    "        self.save_weights(filepath)\n",
    "\n",
    "class GaussianPolicy(BasePolicy):\n",
    "    def __init__(self,hidden_layer_sizes,activation='relu',output_activation='linear', *args, **kwargs):\n",
    "        self._hidden_layer_sizes = hidden_layer_sizes\n",
    "        self._activation = activation\n",
    "        self._output_activation = output_activation\n",
    "\n",
    "        super(GaussianPolicy, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.model = self.feedforward_model(\n",
    "            hidden_layer_sizes=self._hidden_layer_sizes,\n",
    "            output_shape=(self._output_shape, ),\n",
    "            activation=self._activation,\n",
    "            output_activation=self._output_activation\n",
    "        )\n",
    "        # 模型初始化\n",
    "        self.model(self.input_demo[...,np.newaxis])\n",
    "\n",
    "        # self.model = self.feedforward_model(\n",
    "        #     hidden_layer_sizes=self._hidden_layer_sizes,\n",
    "        #     output_shape=(self._output_shape, ),\n",
    "        #     activation=self._activation,\n",
    "        #     output_activation=self._output_activation\n",
    "        # )(np.concatenate(([2.0], [1.0]))[...,np.newaxis])\n",
    "\n",
    "\n",
    "\n",
    "    def feedforward_model(self,hidden_layer_sizes,\n",
    "                          output_shape,\n",
    "                          activation='relu',\n",
    "                          output_activation='softmax',\n",
    "                          name='feedforward_model',\n",
    "                          *args,\n",
    "                          **kwargs):\n",
    "\n",
    "\n",
    "        # 构建连续动作空间的神经网络\n",
    "        output_size = tf.reduce_prod(output_shape)\n",
    "        print(\"hidden_layer_sizes:{},output_shape:{},output_size:{}\".format(hidden_layer_sizes,output_shape,output_size))\n",
    "        if 1 < len(output_shape):\n",
    "            raise NotImplementedError(\"TODO(hartikainen)\")\n",
    "        model = tf.keras.Sequential((\n",
    "            *[\n",
    "                tf.keras.layers.Dense(\n",
    "                    hidden_layer_size, *args, activation=activation, **kwargs)\n",
    "                for hidden_layer_size in hidden_layer_sizes\n",
    "            ],\n",
    "            tf.keras.layers.Dense(\n",
    "                output_size, *args, activation=output_activation, **kwargs)\n",
    "        ), name=name)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def actions(self, observations):\n",
    "        \"\"\"Compute actions for given observations.\"\"\"\n",
    "        action_logits = self.model(observations)\n",
    "        # 不使用Gumbel-max trick\n",
    "        actions = tf.argmax(action_logits,axis=-1)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def log_probs(self, observations, actions):\n",
    "        \"\"\"Compute log probabilities of `actions` given observations.\"\"\"\n",
    "        action_logits = self.model(observations)\n",
    "        # 不使用Gumbel-max trick\n",
    "        actions = tf.argmax(action_logits,axis=-1)\n",
    "\n",
    "        actions_one_hot = tf.one_hot(actions, action_logits.get_shape().as_list()[-1])\n",
    "        actions_prop = tf.reduce_sum(action_logits * actions_one_hot)  # 要求\\pi(a,s)>0\n",
    "        log_probs = tf.math.log(actions_prop)\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def probs(self, observations, actions):\n",
    "        \"\"\"Compute probabilities of `actions` given observations.\"\"\"\n",
    "        action_logits = self.model(observations)\n",
    "        # 不使用Gumbel-max trick\n",
    "        actions = tf.argmax(action_logits,axis=-1)\n",
    "\n",
    "        actions_one_hot = tf.one_hot(actions, action_logits.get_shape().as_list()[-1])\n",
    "        probs = tf.reduce_sum(action_logits * actions_one_hot)  # 要求\\pi(a,s)>0\n",
    "\n",
    "        return probs\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def actions_and_log_probs(self, observations):\n",
    "        \"\"\"Compute actions and log probabilities together.\n",
    "\n",
    "        We need this functions to avoid numerical issues coming out of the\n",
    "        squashing bijector (`tfp.bijectors.Tanh`). Ideally this would be\n",
    "        avoided by using caching of the bijector and then computing actions\n",
    "        and log probs separately, but that's currently not possible due to the\n",
    "        issue in the graph mode (i.e. within `tf.function`) bijector caching.\n",
    "        This method could be removed once the caching works. For more, see:\n",
    "        https://github.com/tensorflow/probability/issues/840\n",
    "        \"\"\"\n",
    "\n",
    "        action_logits = self.model(observations)\n",
    "        # 不使用Gumbel-max trick\n",
    "        actions = tf.argmax(action_logits,axis=-1)\n",
    "        actions_one_hot = tf.one_hot(actions, action_logits.get_shape().as_list()[-1])\n",
    "        actions_prop = tf.reduce_sum(action_logits * actions_one_hot)\n",
    "        log_probs = tf.math.log(actions_prop)\n",
    "\n",
    "        return actions, log_probs\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def actions_and_probs(self, observations):\n",
    "        \"\"\"Compute actions and probabilities together.\n",
    "\n",
    "        We need this functions to avoid numerical issues coming out of the\n",
    "        squashing bijector (`tfp.bijectors.Tanh`). Ideally this would be\n",
    "        avoided by using caching of the bijector and then computing actions\n",
    "        and probs separately, but that's currently not possible due to the\n",
    "        issue in the graph mode (i.e. within `tf.function`) bijector caching.\n",
    "        This method could be removed once the caching works. For more, see:\n",
    "        https://github.com/tensorflow/probability/issues/840\n",
    "        \"\"\"\n",
    "        action_logits = self.model(observations)\n",
    "        # 不使用Gumbel-max trick\n",
    "        actions = tf.argmax(action_logits,axis=-1)\n",
    "        actions_one_hot = tf.one_hot(actions, action_logits.get_shape().as_list()[-1])\n",
    "        probs = tf.reduce_sum(action_logits * actions_one_hot)\n",
    "\n",
    "        return actions, probs\n",
    "\n",
    "    def save_weights(self, *args, **kwargs):\n",
    "        return self.model.save_weights(*args, **kwargs)\n",
    "\n",
    "    def load_weights(self, *args, **kwargs):\n",
    "        return self.model.load_weights(*args, **kwargs)\n",
    "\n",
    "    def get_weights(self, *args, **kwargs):\n",
    "        return self.model.get_weights(*args, **kwargs)\n",
    "\n",
    "    def set_weights(self, *args, **kwargs):\n",
    "        return self.model.set_weights(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return self.model.trainable_weights\n",
    "\n",
    "    @property\n",
    "    def non_trainable_weights(self):\n",
    "        return self.model.non_trainable_weights\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def get_diagnostics(self, inputs):\n",
    "        \"\"\"Return diagnostic information of the policy.\n",
    "\n",
    "        Returns the mean, min, max, and standard deviation of means and\n",
    "        covariances.\n",
    "        \"\"\"\n",
    "        actions, log_pis = self.actions_and_log_probs(inputs)\n",
    "\n",
    "        return OrderedDict((\n",
    "            ('entropy-mean', tf.reduce_mean(-log_pis)),\n",
    "            ('entropy-std', tf.math.reduce_std(-log_pis)),\n",
    "\n",
    "            ('actions-mean', tf.reduce_mean(actions)),\n",
    "            ('actions-std', tf.math.reduce_std(actions)),\n",
    "            ('actions-min', tf.reduce_min(actions)),\n",
    "            ('actions-max', tf.reduce_max(actions)),\n",
    "        ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 环境加载\n",
    "import grid2op\n",
    "from grid2op.Reward import BaseReward, RedispReward, L2RPNSandBoxScore\n",
    "import numpy as np\n",
    "from grid2op.Parameters import Parameters\n",
    "\n",
    "other_rewards = {}\n",
    "other_rewards[\"tmp_score_codalab\"] = L2RPNSandBoxScore\n",
    "input_dir = '../input_data_local'\n",
    "parameters = Parameters()\n",
    "parameters.HARD_OVERFLOW_THRESHOLD = 3.0\n",
    "parameters.MAX_SUB_CHANGED = 6\n",
    "parameters.NB_TIMESTEP_OVERFLOW_ALLOWED = 4\n",
    "parameters.MAX_LINE_STATUS_CHANGED = 100\n",
    "\n",
    "env = grid2op.make(input_dir, param=parameters,\n",
    "                   reward_class=RedispReward,\n",
    "                   other_rewards=other_rewards)\n",
    "env.seed(10)\n",
    "env.set_id(0)\n",
    "obs = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shapes:(2,)\n",
      "hidden_layer_sizes:(50, 50),output_shape:((),),output_size:1.0\n"
     ]
    }
   ],
   "source": [
    "# 处理后的observation\n",
    "observation_space = np.concatenate(([obs.rho.max()], [obs.rho.min()]))\n",
    "# 动作编号\n",
    "action_space = np.array(0)\n",
    "\n",
    "policy = GaussianPolicy(input_demo=observation_space,input_shapes=observation_space.shape, output_shape=action_space.shape,hidden_layer_sizes=(50,50))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.GaussianPolicy object at 0x000001E9E0BEEC08>\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x000001E9ECFEBF08>\n"
     ]
    }
   ],
   "source": [
    "print(policy)\n",
    "print(policy.model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "env_train = grid2op.make(input_dir, param=parameters,\n",
    "                   reward_class=RedispReward,\n",
    "                   other_rewards=other_rewards)\n",
    "env_train.seed(10)\n",
    "env_train.set_id(0)\n",
    "obs_train = env_train.reset()\n",
    "\n",
    "env_evaluate = grid2op.make(input_dir, param=parameters,\n",
    "                         reward_class=RedispReward,\n",
    "                         other_rewards=other_rewards)\n",
    "env_evaluate.seed(10)\n",
    "env_evaluate.set_id(10)\n",
    "obs_evaluate = env_evaluate.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_train:apr_17_1\n",
      "obs_evaluate:jul_13_1\n"
     ]
    }
   ],
   "source": [
    "print(\"obs_train:{}\".format(env_train.chronics_handler.get_name()))\n",
    "\n",
    "print(\"obs_evaluate:{}\".format(env_evaluate.chronics_handler.get_name()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}